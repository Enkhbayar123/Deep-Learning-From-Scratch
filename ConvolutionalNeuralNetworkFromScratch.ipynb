{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V5E1"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qGOeL0MuEJ4j",
        "outputId": "3ad13651-0619-4d13-ae8c-a06e068d902c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow\n",
            "  Downloading tensorflow-2.20.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.5 kB)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.4.0)\n",
            "Collecting astunparse>=1.6.0 (from tensorflow)\n",
            "  Downloading astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
            "Collecting flatbuffers>=24.3.25 (from tensorflow)\n",
            "  Downloading flatbuffers-25.9.23-py2.py3-none-any.whl.metadata (875 bytes)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.6.0)\n",
            "Collecting google_pasta>=0.1.1 (from tensorflow)\n",
            "  Downloading google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
            "Collecting libclang>=13.0.0 (from tensorflow)\n",
            "  Downloading libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl.metadata (5.2 kB)\n",
            "Requirement already satisfied: opt_einsum>=2.3.2 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from tensorflow) (25.0)\n",
            "Requirement already satisfied: protobuf>=5.28.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (6.33.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.32.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.2.0)\n",
            "Requirement already satisfied: typing_extensions>=3.6.6 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (4.15.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.0.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.76.0)\n",
            "Collecting tensorboard~=2.20.0 (from tensorflow)\n",
            "  Downloading tensorboard-2.20.0-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: keras>=3.10.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.10.0)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.0.2)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.15.1)\n",
            "Requirement already satisfied: ml_dtypes<1.0.0,>=0.5.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.5.3)\n",
            "Collecting wheel<1.0,>=0.23.0 (from astunparse>=1.6.0->tensorflow)\n",
            "  Downloading wheel-0.45.1-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from keras>=3.10.0->tensorflow) (14.2.0)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.12/dist-packages (from keras>=3.10.0->tensorflow) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.12/dist-packages (from keras>=3.10.0->tensorflow) (0.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.10.5)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/lib/python3/dist-packages (from tensorboard~=2.20.0->tensorflow) (3.3.6)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.20.0->tensorflow) (12.0.0)\n",
            "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard~=2.20.0->tensorflow)\n",
            "  Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl.metadata (1.1 kB)\n",
            "Collecting werkzeug>=1.0.1 (from tensorboard~=2.20.0->tensorflow)\n",
            "  Downloading werkzeug-3.1.3-py3-none-any.whl.metadata (3.7 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from werkzeug>=1.0.1->tensorboard~=2.20.0->tensorflow) (3.0.3)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.10.0->tensorflow) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.10.0->tensorflow) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.10.0->tensorflow) (0.1.2)\n",
            "Downloading tensorflow-2.20.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (620.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m620.7/620.7 MB\u001b[0m \u001b[31m743.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
            "Downloading flatbuffers-25.9.23-py2.py3-none-any.whl (30 kB)\n",
            "Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl (24.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.5/24.5 MB\u001b[0m \u001b[31m65.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboard-2.20.0-py3-none-any.whl (5.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m86.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m97.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading werkzeug-3.1.3-py3-none-any.whl (224 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading wheel-0.45.1-py3-none-any.whl (72 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: libclang, flatbuffers, wheel, werkzeug, tensorboard-data-server, google_pasta, tensorboard, astunparse, tensorflow\n",
            "Successfully installed astunparse-1.6.3 flatbuffers-25.9.23 google_pasta-0.2.0 libclang-18.1.1 tensorboard-2.20.0 tensorboard-data-server-0.7.2 tensorflow-2.20.0 werkzeug-3.1.3 wheel-0.45.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qtVQ09ak_Xh8",
        "outputId": "dbe5ac64-95af-4798-8086-f6eb956bd63c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jax/_src/cloud_tpu_init.py:86: UserWarning: Transparent hugepages are not enabled. TPU runtime startup and shutdown time should be significantly improved on TPU v5e and newer. If not already set, you may need to enable transparent hugepages in your VM image (sudo sh -c \"echo always > /sys/kernel/mm/transparent_hugepage/enabled\")\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. Loading MNIST data from Keras library...\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Original X_train shape: (60000, 28, 28)\n",
            "Original X_test shape: (10000, 28, 28)\n",
            "\n",
            "--- Final Shapes ---\n",
            "X_train: (48000, 28, 28, 1)\n",
            "y_train: (48000, 10)\n",
            "X_val:   (12000, 28, 28, 1)\n",
            "y_val:   (12000, 10)\n",
            "X_test:  (10000, 28, 28, 1)\n",
            "\n",
            "Library load successful!\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def load_data_library(val_split=0.2):\n",
        "    print(\"1. Loading MNIST data from Keras library...\")\n",
        "\n",
        "    (X_train_full, y_train_full), (X_test_raw, _) = mnist.load_data()\n",
        "\n",
        "    print(f\"Original X_train shape: {X_train_full.shape}\")\n",
        "    print(f\"Original X_test shape: {X_test_raw.shape}\")\n",
        "\n",
        "    X_train_full = X_train_full.astype(\"float32\") / 255.0\n",
        "    X_test = X_test_raw.astype(\"float32\") / 255.0\n",
        "\n",
        "    X_train_full = X_train_full.reshape(-1, 28, 28, 1)\n",
        "    X_test = X_test.reshape(-1, 28, 28, 1)\n",
        "\n",
        "    y_train_full_oh = to_categorical(y_train_full, 10)\n",
        "\n",
        "    X_train, X_val, y_train, y_val = train_test_split(\n",
        "        X_train_full,\n",
        "        y_train_full_oh,\n",
        "        test_size=val_split,\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "    print(\"\\n--- Final Shapes ---\")\n",
        "    print(f\"X_train: {X_train.shape}\")\n",
        "    print(f\"y_train: {y_train.shape}\")\n",
        "    print(f\"X_val:   {X_val.shape}\")\n",
        "    print(f\"y_val:   {y_val.shape}\")\n",
        "    print(f\"X_test:  {X_test.shape}\")\n",
        "\n",
        "    return (X_train, y_train), (X_val, y_val), X_test\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    (X_train, y_train), (X_val, y_val), X_test = load_data_library()\n",
        "    print(\"\\nLibrary load successful!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class Layer:\n",
        "    def __init__(self):\n",
        "        self.input = None\n",
        "\n",
        "    def forward(self, input):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def backward(self, output_gradient):\n",
        "        raise NotImplementedError\n",
        "\n",
        "class Conv2D(Layer):\n",
        "    def __init__(self, input_channels, num_filters, filter_size, stride=1, padding=0):\n",
        "        super().__init__()\n",
        "        self.num_filters = num_filters\n",
        "        self.filter_size = filter_size\n",
        "        self.stride = stride\n",
        "        self.padding = padding\n",
        "\n",
        "        scale = np.sqrt(2. / (filter_size * filter_size * input_channels))\n",
        "        self.weights = np.random.randn(\n",
        "            num_filters, filter_size, filter_size, input_channels\n",
        "        ) * scale\n",
        "        self.biases = np.zeros(num_filters)\n",
        "\n",
        "    def forward(self, input):\n",
        "        self.input = input\n",
        "        (batch_size, h_in, w_in, c_in) = input.shape\n",
        "        if self.padding > 0:\n",
        "            self.input_padded = np.pad(\n",
        "                input,\n",
        "                ((0, 0), (self.padding, self.padding), (self.padding, self.padding), (0, 0)),\n",
        "                'constant'\n",
        "            )\n",
        "        else:\n",
        "            self.input_padded = input\n",
        "\n",
        "        (batch_size, h_pad, w_pad, c_in) = self.input_padded.shape\n",
        "        h_out = (h_pad - self.filter_size)\n",
        "        w_out = (w_pad - self.filter_size)\n",
        "        output = np.zeros((batch_size, h_out, w_out, self.num_filters))\n",
        "        for b in range(batch_size):\n",
        "            for y  in range(h_out):\n",
        "                for x in range(w_out):\n",
        "                    y_start = y * self.stride\n",
        "                    y_end = y_start + self.filter_size\n",
        "                    x_start = x * self.stride\n",
        "                    x_end = x_start + self.filter_size\n",
        "                    input_slice = self.input_padded[b, y_start:y_end, x_start:x_end, :]\n",
        "                    for f in range(self.num_filters):\n",
        "                        current_filter = self.weights[f]\n",
        "                        current_bias = self.biases[f]\n",
        "                        conv_val = np.sum(input_slice * current_filter) + current_bias\n",
        "                        output[b, y, x, f] = conv_val\n",
        "\n",
        "        return output\n",
        "    def backward(self, output_gradient):\n",
        "        (batch_size, h_out, w_out, num_filters) = output_gradient.shape\n",
        "\n",
        "        d_input = np.zeros_like(self.input)\n",
        "        d_input_padded = np.zeros_like(self.input_padded)\n",
        "        self.d_weights = np.zeros_like(self.weights)\n",
        "        self.d_biases = np.zeros_like(self.biases)\n",
        "        for f in range(num_filters):\n",
        "            self.d_biases[f] = np.sum(output_gradient[:, :, :, f])\n",
        "        for b in range(batch_size):\n",
        "            for y in range(h_out):\n",
        "                for x in range(w_out):\n",
        "                    y_start = y * self.stride\n",
        "                    y_end = y_start + self.filter_size\n",
        "                    x_start = x * self.stride\n",
        "                    x_end = x_start + self.filter_size\n",
        "                    input_slice = self.input_padded[b, y_start:y_end, x_start:x_end, :]\n",
        "                    for f in range(num_filters):\n",
        "                        grad = output_gradient[b, y, x, f]\n",
        "                        self.d_weights[f] += input_slice * grad\n",
        "                        d_input_padded[b, y_start:y_end, x_start:x_end, :] += self.weights[f] * grad\n",
        "        if self.padding > 0:\n",
        "            d_input = d_input_padded[:, self.padding:-self.padding, self.padding:-self.padding, :]\n",
        "        else:\n",
        "            d_input = d_input_padded\n",
        "        return d_input\n",
        "\n",
        "class MaxPooling(Layer):\n",
        "    def __init__(self, pool_size, stride):\n",
        "        super().__init__()\n",
        "        self.pool_size = pool_size\n",
        "        self.stride = stride\n",
        "        self.debug_printed = False # Flag to print only once\n",
        "\n",
        "    def forward(self, input):\n",
        "        self.input = input\n",
        "        (batch_size, h_in, w_in, c_in) = input.shape\n",
        "\n",
        "        # --- DEBUGGING BLOCK ---\n",
        "        if not self.debug_printed:\n",
        "            print(f\"DEBUG: MaxPooling Input Shape: {input.shape}\")\n",
        "            self.debug_printed = True\n",
        "        # -----------------------\n",
        "\n",
        "        h_out = (h_in - self.pool_size) // self.stride + 1\n",
        "        w_out = (w_in - self.pool_size) // self.stride + 1\n",
        "\n",
        "        # Safety Check\n",
        "        if h_out <= 0 or w_out <= 0:\n",
        "            raise ValueError(f\"Output dimension is zero/negative! h_in={h_in}, pool={self.pool_size}\")\n",
        "\n",
        "        output = np.zeros((batch_size, h_out, w_out, c_in))\n",
        "\n",
        "        for b in range(batch_size):\n",
        "            for c in range(c_in):\n",
        "                for y in range(h_out):\n",
        "                    for x in range(w_out):\n",
        "                        y_start = y * self.stride\n",
        "                        y_end = y_start + self.pool_size\n",
        "                        x_start = x * self.stride\n",
        "                        x_end = x_start + self.pool_size\n",
        "\n",
        "                        input_slice = input[b, y_start:y_end, x_start:x_end, c]\n",
        "\n",
        "                        # FIX: Check if slice is empty before calling max\n",
        "                        if input_slice.size == 0:\n",
        "                             print(f\"ERROR at b={b}, y={y}, x={x}. Slice indices: {y_start}:{y_end}, {x_start}:{x_end}\")\n",
        "                             print(f\"Input shape was: {input.shape}\")\n",
        "                             return output # Return incomplete output to avoid crash\n",
        "\n",
        "                        output[b, y, x, c] = np.max(input_slice)\n",
        "        return output\n",
        "\n",
        "    def backward(self, output_gradient):\n",
        "        # (Use the same backward code as Step 3)\n",
        "        (batch_size, h_in, w_in, c_in) = self.input.shape\n",
        "        (batch_size, h_out, w_out, c_in) = output_gradient.shape\n",
        "        d_input = np.zeros_like(self.input)\n",
        "\n",
        "        for b in range(batch_size):\n",
        "            for c in range(c_in):\n",
        "                for y in range(h_out):\n",
        "                    for x in range(w_out):\n",
        "                        y_start = y * self.stride\n",
        "                        y_end = y_start + self.pool_size\n",
        "                        x_start = x * self.stride\n",
        "                        x_end = x_start + self.pool_size\n",
        "                        input_slice = self.input[b, y_start:y_end, x_start:x_end, c]\n",
        "                        max_val = np.max(input_slice)\n",
        "                        mask = (input_slice == max_val)\n",
        "                        d_input[b, y_start:y_end, x_start:x_end, c] += mask * output_gradient[b, y, x, c]\n",
        "        return d_input\n",
        "\n",
        "class ReLU(Layer):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "    def forward(self, input):\n",
        "        self.input = input\n",
        "        return np.maximum(0, input)\n",
        "    def backward(self, output_gradient):\n",
        "        relu_grad = (self.input > 0).astype(float)\n",
        "        return output_gradient * relu_grad\n",
        "\n",
        "class Flatten(Layer):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.input_shape = None\n",
        "\n",
        "    def forward(self, input):\n",
        "        self.input_shape = input.shape\n",
        "        batch_size = input.shape[0]\n",
        "        return input.reshape(batch_size, -1)\n",
        "    def backward(self, output_gradient):\n",
        "        return output_gradient.reshape(self.input_shape)\n",
        "\n",
        "class Dense(Layer):\n",
        "    def __init__(self, input_size, output_size):\n",
        "        super().__init__()\n",
        "        scale = np.sqrt(2. / input_size)\n",
        "        self.weights = np.random.randn(input_size, output_size) * scale\n",
        "        self.biases = np.zeros((1, output_size))\n",
        "    def forward(self, input):\n",
        "        self.input = input\n",
        "        return np.dot(input, self.weights) + self.biases\n",
        "    def backward(self, output_gradient):\n",
        "        self.d_weights = np.dot(self.input.T, output_gradient)\n",
        "        self.d_biases = np.sum(output_gradient, axis=0, keepdims=True)\n",
        "        d_input = np.dot(output_gradient, self.weights.T)\n",
        "        return d_input\n",
        "\n",
        "class Softmax(Layer):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.output = None\n",
        "    def forward(self, input):\n",
        "        stable_input = input - np.max(input, axis=1, keepdims=True)\n",
        "        exp_scores = np.exp(stable_input)\n",
        "        self.output = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
        "        return self.output\n",
        "    def backward(self, output_gradient):\n",
        "        pass"
      ],
      "metadata": {
        "id": "iZLXB_hIAg4C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SoftmaxCrossEntropy:\n",
        "    def __init__(self):\n",
        "        self.y_pred = None\n",
        "        self.y_true = None\n",
        "    def loss(self, logits, y_true):\n",
        "        self.y_true = y_true\n",
        "        exps = np.exp(logits - np.max(logits, axis=1, keepdims=True))\n",
        "        self.y_pred = exps / np.sum(exps, axis=1, keepdims=True)\n",
        "        epsilon = 1e-12\n",
        "        self.y_pred = np.clip(self.y_pred, epsilon, 1. - epsilon)\n",
        "        correct_logprobs = -np.log(self.y_pred[range(len(y_true)), y_true.argmax(axis=1)])\n",
        "        data_loss = np.mean(correct_logprobs)\n",
        "        return data_loss\n",
        "    def backward(self):\n",
        "        d_logits = self.y_pred - self.y_true\n",
        "        d_logits = d_logits / len(self.y_true)\n",
        "        return d_logits"
      ],
      "metadata": {
        "id": "sGpmNvFzAvky"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import time\n",
        "\n",
        "class SGD:\n",
        "    def __init__(self, learning_rate=0.01):\n",
        "        self.learning_rate = learning_rate\n",
        "    def update(self, layer):\n",
        "        if hasattr(layer, 'weights'):\n",
        "            layer.weights -= self.learning_rate * layer.d_weights\n",
        "            layer.biases -= self.learning_rate * layer.d_biases\n",
        "\n",
        "def train(network, loss_layer, X_train, y_train, epochs=5, batch_size=32, learning_rate=0.01):\n",
        "    optimizer = SGD(learning_rate)\n",
        "    print(f\"Training on {len(X_train)} samples...\")\n",
        "    for epoch in range(epochs):\n",
        "        start_time = time.time()\n",
        "        epoch_loss = 0\n",
        "        indices = np.arange(len(X_train))\n",
        "        np.random.shuffle(indices)\n",
        "        X_train = X_train[indices]\n",
        "        y_train = y_train[indices]\n",
        "        for i in range(0, len(X_train), batch_size):\n",
        "            X_batch = X_train[i:i+batch_size]\n",
        "            y_batch = y_train[i:i+batch_size]\n",
        "            output = X_batch\n",
        "            for layer in network:\n",
        "                output = layer.forward(output)\n",
        "            loss = loss_layer.loss(output, y_batch)\n",
        "            epoch_loss += loss\n",
        "            grad = loss_layer.backward()\n",
        "            for layer in reversed(network):\n",
        "                grad = layer.backward(grad)\n",
        "            for layer in network:\n",
        "                optimizer.update(layer)\n",
        "        avg_loss = epoch_loss / (len(X_train) // batch_size)\n",
        "        duration = time.time() - start_time\n",
        "        print(f\"Epoch {epoch + 1}/{epoch} - Loss: {avg_loss:.4f} - Time: {duration:.2f}s\")\n",
        "\n",
        "def predict(network, X):\n",
        "    output = X\n",
        "    for layer in network:\n",
        "        output = layer.forward(output)\n",
        "    exps = np.exp(output - np.max(output, axis=1, keepdims=True))\n",
        "    probs = exps / np.sum(exps, axis=1, keepdims=True)\n",
        "    return np.argmax(probs, axis=1)\n",
        "\n",
        "def accuracy(network, X, y_true_one_hot):\n",
        "    preds = predict(network, X)\n",
        "    y_true = np.argmax(y_true_one_hot, axis=1)\n",
        "    return np.mean(preds == y_true)\n",
        "\n",
        "\n",
        "network = [\n",
        "    Conv2D(input_channels=1, num_filters=8, filter_size=3, stride=1, padding=0),\n",
        "    ReLU(),\n",
        "    MaxPooling(pool_size=2, stride=2),\n",
        "    Flatten(),\n",
        "    Dense(input_size=1152, output_size=10)\n",
        "]\n",
        "loss_layer = SoftmaxCrossEntropy()\n",
        "\n",
        "print(\"Starting training... (This might be slow because it's pure NumPy!)\")\n",
        "\n",
        "train(\n",
        "    network,\n",
        "    loss_layer,\n",
        "    X_train,\n",
        "    y_train,\n",
        "    epochs=3,\n",
        "    batch_size=32,\n",
        "    learning_rate=0.05\n",
        ")\n",
        "\n",
        "print(\"\\nCalculating VAlidation Accuracy...\")\n",
        "acc = accuracy(network, X_val, y_val)\n",
        "print(f\"Validation Accuracy: {acc * 100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2SDrQl6iAzU-",
        "outputId": "c2440792-b0d6-4c91-c534-35a1eedbddcd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training... (This might be slow because it's pure NumPy!)\n",
            "Training on 48000 samples...\n",
            "DEBUG: MaxPooling Input Shape: (32, 25, 25, 8)\n",
            "Epoch 1/0 - Loss: 0.3441 - Time: 3346.83s\n",
            "Epoch 2/1 - Loss: 0.1743 - Time: 3320.07s\n"
          ]
        }
      ]
    }
  ]
}