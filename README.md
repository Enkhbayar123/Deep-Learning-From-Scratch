# Deep-Learning-From-Scratch
A Python implementation of Deep Learning models (MLP and CNN) built entirely from first principles using NumPy.  This repository avoids high-level frameworks like PyTorch or TensorFlow to demonstrate a fundamental understanding of the mathematics behind neural networks, including backpropagation, gradient descent, and tensor operations.

üìÇ Project StructureThe repository contains two main implementations applied to the Kaggle Digit Recognizer (MNIST) dataset:mlp.py (Multi-Layer Perceptron): A standard fully connected feedforward network.cnn.py (Convolutional Neural Network): A complete CNN implementation including custom forward and backward passes for Convolution and Pooling layers.üöÄ Features implementedMatrix Operations: Efficient vectorized forward/backward passes using NumPy.Custom Layers:Dense (Fully Connected)Conv2D (Convolutional with padding/stride)MaxPooling (Downsampling)FlattenActivation Functions:ReLU (Rectified Linear Unit)Softmax (Probability distribution)Loss Function:Categorical Cross-Entropy (with numerical stability optimizations)Optimization:Stochastic Gradient Descent (SGD)üßÆ The MathematicsThe core of this project is the manual implementation of Backpropagation.Dense Layers: Implemented using chain rule matrix multiplication ($\frac{\partial E}{\partial W} = X^T \cdot \frac{\partial E}{\partial Y}$).Convolutional Layers: Implemented "Transposed Convolution" logic to propagate gradients through filters to the input map.Pooling Layers: Implemented gradient routing (passing the gradient only to the "winning" max pixel).üõ†Ô∏è Installation & UsageClone the repository:Bashgit clone https://github.com/yourusername/Deep-Learning-From-Scratch.git
Install dependencies:Bashpip install numpy pandas sklearn
Download Data:Download train.csv and test.csv from the Kaggle Digit Recognizer Competition.Place them in the root directory.Run the Training:Bashpython cnn.py
